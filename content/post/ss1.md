---
title: "스파크를 활용한 실시간 처리 : ch1. 스트림 처리 소개"
date: 2021-09-08T20:58:42+09:00
draft: false
categories: ["spark", "streaming"]
tags: ["Spark", "Stream"]
---

책 스파크를 활용한 실시간 처리 1단원 정리입니다.

## 스트림 처리(stream processing)가 무엇 인지?

스트림 처리는 unbounded data( 크기가 무한한 유형의 데이터 셋)로 부터 정보를 추출하는데 사용하는 규율 및 관련 기술의 집합으로 정의된다.

스트림 처리는 데이터가 시스템에 도착할 때 윈도우 내 데이터 또는 최신의 데이터 쿼리하거나 처리 하는데에 주로 집중한다. 반대 되는 개념으로 일괄 처리(batch processing)가있는데 bounded data(알려진 크기의 데이터셋)의 모든 또는 대부분 데이터를 쿼리하거나 처리하는 것이다.

스트림 처리 프로그램은 입력을 시간의 흐름에 따라 관찰되는 무한 길이의 신호 시퀀스라고 가정한다.

여기에서 데이터의 발생을 두 가지로 분류할 수 있는데 

- 유휴 데이터 : 파일, DB, 기타 등등 ( 과거의 데이터 )
- 사용 중인 데이터 : 센서와 같이 연속적으로 생성되는 시퀀스

    또한 사용 중인 데이터에는 두가지 시간 개념이 존재한다. 

    - 이벤트 시간 : 이벤트 발생으로 데이터가 생성될 당시의 로컬 타임
    - 처리 시간 : 시스템에 의해 처리될 당시의 시간

    데이터의 정렬 및 집계 시 이 두 시간이 더욱 중요해진다.

스트림 처리의 예제 : 가능한 최신의 데이터를 소비하는 경우가 좋을 경우의 예시이다.

1. 장치 모니터링
2. 고장 탐지
3. 보험 청구 시스템
4. 차량 관리
5. 미디어 추천
6. 대출 서비스 

## 대용량 데이터 처리에 대한 소개 : Map Reduce

맵 리듀스는 구글에서 분산 병렬 컴퓨팅에서 대용량 데이터  처리를 위한 목적으로 만든 프레임워크이다. 프로그래밍 API이자 구성 요소 집합으로 분산 시스템에 대한 프로그래밍을 모든 이전 작업보다 상대적으로 쉽게 만들었다. 

맵 리듀스의 핵심은 다음 두 함수에 있다. 아래 두 함수의 조합으로 데이터 셋에서 수행하려는 모든 연산을 표현할 수 있다.

{{< figure src="/images/sparkstream/ch1.0.png" title="Map Reduce" >}}

### Map

분산 파일 시스템으로 분산된 컬렉션의 청크를 머신이 읽는다. 읽은 청크에 적용할 함수를 인수로 받아 처리하고 결과를 내보낸다.

### Reduce

맵으로 처리한 컬렉션을 가져와서 새로운 집계로 묶는 연산을 해서 새로운 결과를 집계한다. 

### 맵 리듀스 모델의 중요한 속성 두가지

- 확장성 : 데이터 셋이 증가 시 클러스터에 더 많은 리소스를 추가해서 안정적인 처리 성능을 보장할 수 있다.
- 결함 허용 : 모든 데이터를 복제해서 부분적인 장애 발생 시 손상된 작업만 다시 시작해서 복구가 가능하다.

## 아파치 스파크

대규모 데이터 처리를 위한 빠르고 안정적이며 내결함성(fault-tolerance)이 있는 분산 컴퓨팅 프레임워크로 Disk I/O 기반의 맵 리듀스 였던 하둡(Hadoop)과는 다르게 메모리에 데이터 처리를 캐싱하기 때문에 처리 속도가 최대 100배 빠르다. 

스파크 RDD(Resilient Distributed Dataset, 탄력적 분산 데이터셋)는 클러스터에서 분산 컴퓨팅의 복잡성을 추상화하는 풍부한 기능적 프로그래밍 모델을 제공한다.

데이터 지연 변환을 표현하는 transformation과 계산을 구체화 하는 action을 도입해서 표현력을 강화했고 사용 진입 장벽을 낮출 수 있는 스파크 SQL 그리고 DataFrame와 DataSet 도입했다.

분석 엔진으로 스칼라, 자바, 파이썬 그리고 R언어 API 제공으로 polyglot한 접근이 가능하다.

### 스파크 컴포넌트

스파크는 여러가지 컴포넌트로 구성된 플랫폼이다.

{{< figure src="/images/sparkstream/ch1.1.png" title="Spark Echo System" >}}

- 스파크 코어

    스파크 핵심 실행 엔진과 컴퓨팅 리소스 클러스터 연산을 배포하는데 사용하는 하위 레벨의 기능적 API 셋을 포함하고 있다. 

    추상화된 클러스터 워크로드로 YARN, Mesos 그리고 Kubernetes에 자체 독립형 클러스터 모드를 사용할 수 있다.

    데이터 소스 추상화로 파일, 블록 저장소, DB 및 이벤트 브로커와 같은 데이터 공급자를 통합 사용 가능하다.

- 스파크 SQL

    DataSet 그리고 DataFrame API를 구현하고 SQL을 추가 지원한다. 

    Catalyst 쿼리 엔진과 Tungsten 프로젝트의 코드 생성 및 메모리 관리로 성능 최적화를 했다.

- MLLib ( 머신러닝 ), GraphFrame( 그래프 분석 )

## 스파크 스트리밍

스파크 엔진의 분산 처리 기능 위에 구축된 스트림 처리 프레임워크이다.

개발 전제가 다음과 같다고 한다. '스파크의 분산 컴퓨팅 기능을 적용하여 연속적인 데이터 스트림을 스파크가 작동할 수 있는 개별 데이터 컬렉션으로 변환하여 스트림 처리에 적용하라' (Micro Batch Model)

DStream(Discretized Stream)을 도입하여 프로그래밍 모델을 스트림 기본 데이터에서 동작하도록 한다.

## 구조적 스트리밍

스파크 SQL 웨에 구축된 스트림 프로세서로 데이터셋 과 데이터프레임을 스트리밍 기능으로 확장한 것이다.

선언적 모델을 사용해서 스트림이나 스트림셋에서 데이터를 수집한다. 기존 데이터셋과 데이터프레임 API에서 제공하는 일반 변환 모델을 지원할 뿐만 아니라 이벤트 시간, 스트리밍 조인 및 기본 런타임과의 분리와 같은 스트림 별 기능을 제공한다.

기본 구현은 마이크로배치 방식을 사용하지만, 스파크 2.3 이후로는 실시간 연속 실행 모드를 지원한다.