---
title: "GPT2 : Language Models are Unsupervised Multitask Learners"
date: 2021-04-24T09:33:15+09:00
draft: false
categories: ["paper-review"]
tags: ["ai", "nlp", "gpt"]
---

# Language Models are Unsupervised Multitask Learners

> Language Model 이란?

    언어를 모델링 하기 위해 단어 시퀀스(문장)에 확률을 할당 하는 모델  ( m 개의 단어 시퀀스가 나타날 확률)

    잘 학습된 언어 모델은 입력 되는 단어 시퀀스(문장) 이 자연스러울 수록 더 높은 확률을 부여한다.

> Unsupervised Learning 이란?

    입력 값과 타겟 값이 주어지지 않은 환경에서 학습

> Multi-Task Learning 이란?

    서로 연관 있는 과제들을 동시에 학습함으로써 모든 과제 수행의 성능을 전반적으로 향상시키려는 학습 패러다임

# Abstract

- Web Text 라는 거대한 데이터 셋에 비지도 학습을 사용해서 언어 모델을 학습 했다.
- CoQA 데이터셋에 학습 예제를 사용하지 않고 기존 4개 시스템 중 3개 보다 좋거나 비슷한 성능을 보였다.
- 언어 모델의 파라미터 수는 제로 zero-shot transfer 에 필수적이다. 파라미터 수를 늘릴 수록 전체 태스크에 대해 log-linear 하게 성능이 향상된다.
- GPT-2의 가장 큰 모델은 1.5B 파라미터 수를 가진 트랜스포머다.
- 언더피팅 임에도 불구하고 8개 언어 모델 테스트 데이터 셋에 대해 7개 SOTA를 했다.
- 연구를 통해 자연스럽게 발생하는 태스크 예시로 부터 언어 처리 시스템을 구축하는 훌륭한 방법을 제공한다.

{{< figure src="/images/gpt2/1.png" title="1" >}}

# 1. Introduction

- 특정 태스크에 대해 큰 데이터셋 + 고용량 모델 + 지도 학습으로 현재의 학습한 시스템은 유능한 일반인 보다는 협소한 전문가라고 할 수 있다.
- 수동으로 학습 데이터셋을 생성하지 않고도 일반적인 많은 태스크를 수행하는 시스템을 만들기를 원함.
- 다양한 종류의 입력이 들어가는 태스크의 경우 일정한 성능을 보이지 못하는데, 특정 도메인 데이터셋을 사용한 특정 태스크 학습이 일반화 부족의 원인이라고 본 논문에서 본다. 안정성 있는 모델을 위해 많은 태스크와 도메인에 대해 학습과 성능 측정을 할 필요가 있고 최근 연구로 GLUE나  decaNLP가 있다.
- 멀티 태스크 러닝은 아직이다. ( 데이터셋, 목적 )의 쌍을 학습하는 형태, 메타 러닝의 관점에서는 위의 (데이터셋, 목적) 쌍에서 샘플링 된 한 훈련 데이터다. 멀티 태스크 러닝은 이 쌍을 학습하기 위한 효과적인 학습 예제가 필요하고 데이터셋을 확장해 나가기가 어렵다. 그리고 그 objective로 어느 정도로 도달 하게 설계하기가 어렵다. 본 논문에서 멀티 태스크 러닝을 위한 추가적인 셋업을 탐색해 본다.
- 현재는 사전 학습과 지도 학습 파인 튜닝을 합쳐서 언어 태스크에 이용한다.

        1. 워드 벡터를 학습해서 태스크에 특화된 구조에서 입력으로 사용한다.

        2. RNN 구조 트랜스퍼 된다.

        3. 셀프 어텐션 블록  트랜스퍼로

- 지도 데이터가 없거나 적은 경우에 다른 방식을 사용하는 언어 모델이 common sense reasoning( 상식 추론 ), sentiment analysis( 감정 분석 ) 같은 태스크를 수행 할 수 있음을 증명한다.
- 본 논문에서는 두 방법을 합치고 일반적인 트랜스퍼 메소드를 사용한다.
- 제로 샷 세팅에서 언어 모델이 파라미터나 구조 변경 없이 다운 스트림 태스크를 훌륭하게 수행할 수 있음을 태스크에 따라선 경쟁력 있음, 훌륭함, SOTA 라는 것을 성능으로 증명한다.

# 2. Approach

언어 모델링은 심볼의 시퀀스$( s_1, s_2, ..., s_n )$로 구성된 예제 셋$( x_1, x_2, ..., x_n )$ 의 비지도 분포 추정으로 특정 될 수 있다. 언어는 순서가 있기 때문에,

{{< figure src="/images/gpt2/2.png" title="2" >}}

 조건부 확률의 곱으로 이루어진 심볼에 대해 결합 확률

위의 식으로 구성하는 것이 일반적이다. 그리고 이 접근법은 

1. p(x)를 추정 할 수 있게 한다.
2. p(x)로 부터 샘플을 추출 할 수 있게도 한다. 
3.  $p(s_{n-k}, ..., s_n | s_1, ..., s_{n-k-1})$ 의 조건부 확률도 할 수 있게 한다.

→ 셀프 어텐션 구조인 트랜스포머는 이러한 조건부 확률을 계산해서 표현하는데 훌륭한 성능을 보인다.

- 다양한 태스크를 수행하는 모델이 필요해서 $p(output | input, task)$를 모델링 해야한다.
- 태스크 컨디셔닝(과제에 조건을 다는 것)은 세 Level에서 구현 할 수 있다.
    1. 구조 level ( 특정 태스크에 맞춘 인코더-디코더 구조 )
    2. 알고리즘 level ( inner 와 outer loop 의 최적화 구조인 MAML  알고리즘 )
    3. McCann 은 ( task, input, output)으로  한다. MQAN으로 이 포멧이 가능함을 보였다.

{{< figure src="/images/gpt2/3.png" title="3" >}}

{{< figure src="/images/gpt2/4.png" title="4" >}}

- 언어 모델은 McCann을 태스크를 명시적인 라벨이 없어도 학습이 가능하다. 토이 세팅에서 지도 학습에 비해 학습이 많이 느렸다

    심볼의 부분 집합에 대해 평가를 할 때에는 지도 학습의 목적 함수와 비지도 학습의 목적 함수가 같아서, 글로벌 미니멈도 같다. 학습 할 수 있다는 얘기다.

    비지도 학습의 목적 함수는 지도 학습의 목적 함수와 다를 수 있다( Sutskever et al. 2015 ) 는 것은 논외로. 비지도 학습의 목적 함수를 수렴 시킬 수 있는지가 관건 이었다.

- 대화의 맥락에서 자연어에서 직접 학습 할 수 있는 시스템 개발 필요성을 주장하고 개념을 증명한 사례가 있다. 대화는 흥미롭지만, 제한적이라는 우려가 있었다. 어차피 인터넷 데이터는 양이 많기도 하다.

    QA 태스크의 학습 forward prediction으로 보상 신호 없이 교사 출력을 사용해서 

- 충분한 용량의 모델은 데이터의 조달에 관계 없이 사실상 비지도 학습으로 자연어에서 증명된 태스크 수행과 추론을 학습할 것이라는 가정 이 있었다.

## 2.1. Training Dataset

- 본 논문의 접근 방법은 가능한  다양한 도메인과 컨텍스트에서 태스크에 대한 자연어 데모를 수집하기 위해 크고 다양한 데이터 셋을 구축이 필요하다.
- 사람에게서 큐레이트 된 데이터 만으로 웹 스크랩 데이터 셋을 새로 만들었다.

    Common Crawl은 유명한 데이터 셋으로 다양하고 굉장히 많은 웹 스크랩  소스 지만, 퀄리티 문제 그리고 중복 문제가 있다

- 레딧에서 휴리스틱하게 3 karma 이상을 스크랩 했다고 한다. 45 백만 링크,
- WebText 로 17년 12월 링크와 위키피디아 문서, 중복 문서는 지워 8 백만 문서에 40GB 의 텍스트이다.

## 2.2. Input Representation

- 언어 모델은 어떠한 string에 대해서도 확률을 구할 수 있어야 한다.
- 현재 거대한 크기의 LM(언어 모델)은 전처리 과정( 대소문자, 토큰화, 사전 외 토큰 )으로 모델이 사용 가능한 문자열을 제한한다.
- Unicode 문자열을 UTF-8 로 처리한다.
- 성능 기준 으로 단어 수준(자주 발생하는 symbol) 언어 모델 > 바이트 수준(자주 발생하지 않는) 언어 모델

    [BPE](https://www.notion.so/BPE-e1430fd82f164a56bff3419e5ae71a1f)

- 그래서 유니코드 단위의 토큰을 처리하는데 ... 유니코드 수준의 BPE는 13만 개의 토큰을 처리한다. (보통 3.2만 ~ 6.4만의 토큰을 처리 ) 반면 바이트 수준의 BPE는 256의 토큰을 처리
- 바이트 시퀀스에 직접 BPE를 처리하는 것이 다루는 토큰 수가 적지만, greedy frequency를 처리하기 때문에 최적이 아니게 된다. ( dog 가 dog. dog! dog? 등을 가질 수 있게 된다. )
- 단어 수준을 넘어서 병합을 못하게 제한. 그치만, 압축 효율성을 크게 증가시키는 것은 예외로 두었다.

→ 단어 레벨 언어 모델의 경험적 이점과 문자 수준 접근법의 일반성을 결합하게 한다. 어떤 유니코드 문자열에서도 확률을 부여할 수 있어서 어떤 dataset에서도 모델을 평가할 수 있게 한다.

## 2.3. Model

{{< figure src="/images/gpt2/5.png" title="5" >}}

Transformer가 기본 구조이며 gpt-1 의 구조를 그대로 따른다.

- Layer Norm이 pre-activation residual-network 처럼 각 sub-block의 입력으로 옮겨졌다.
- 모델 깊이에 따른 residual path의 누적에 관한 부분의 초기화 방법이 $1 / \sqrt{N}$ 곱으로 변경 되었다.
- vocab 이 50,257로 확장 되었다.
- context-size가 512 - 1024 개 토큰으로 늘어났고, 배치 사이즈가 512로 증가 되었다.

{{< figure src="/images/gpt2/6.png" title="6" >}}

{{< figure src="/images/gpt2/7.png" title="7" >}}

{{< figure src="/images/gpt2/8.png" title="8" >}}

GPT-1 > BERT > GPT-2 > GPT-2

# 3. Experiment

- 각 모델의 학습율은 WebText의 5%를 떼어 만든 held-out 샘플을 사용해서 수동으로 튜닝했다. 모든 모델이 언더피팅 되었고 학습 시간을 늘릴 수록 성능이 개선 되었다.

## 3.1. Language Modeling

- WebText 언어 모델의 성능을 알기 위해 기본 학습에 대해 제로샷으로 도메인에 적용했다.
- 바이트 단위라 전처리나 토큰화가 필요하지 않고 모든 데이터셋에 적용이 가능하다.
- 성능 평가는 표준 예측 단위 마다 negative log probability의 평균으로 사용 했다.
- 일반적인 분포를 벗어난 규격화된 텍스트, 분리된 구두점이나 축약점, 섞인 문장에 대해 평가했다.
- 400억 바이트에서 <UNK>는 26번 밖에 발생하지 않았다.

{{< figure src="/images/gpt2/9.png" title="9" >}}

1542M 모델에서 8 개의 데이터 셋 가운데 7개 데이터 셋에 SOTA를 달성 했다.

## 3.2. Children's Boot Test

{{< figure src="/images/gpt2/10.png" title="10" >}}

고유 명사, 명사, 동사, 전치사에 따른 성능을 측정하기 위해 진행 되었다. 10개의 가능한 선택지와 나머지 문장의 확률을 계산하고 확률이 가장 높은 문장을 선택해서 정확도를 계산했다. 일부 겹치는게 있었는데 중복이 적은 검증 셋에 대한 결과를 보고 했는데, 두 부분에서 SOTA를 했다.

## 3.3. LAMBADA

텍스트의 장기 의존성(시퀀스가 길 때, 과거를 잘 반영하지 못하는 )에 대해 모델링 하는 능력을 테스트 했다. 마지막 단어를 예측하는 태스크다. stop-word를 넣으면 성능이 더 올라갔고. 당연히 SOTA 했다.

## 3.4. Winograd Schema Challenge

{{< figure src="/images/gpt2/11.png" title="11" >}}

텍스트에 존재하는 중의성을 다루는 능력을 측정해서 일반 상식 추론 능력을 검증한다. SOTA 했다.

## 3.5. Reading Comprehension

CoQA 데이터셋 에 대해서 독해 능력을 평가했다. 12.7 만의 학습 데이터를 사용하지 않고도 SOTA 했다.

## 3.6. Summarization

{{< figure src="/images/gpt2/12.png" title="12" >}}

TL : Too Long // DR : Didn;t Read

CNN과 Daily Mail 데이터 셋에 요약 성능을 테스트 했다. Top-2 랜덤 샘플링에서 100개의 토큰을 생성하고 거기서 처음 생성된 3개의 문장을 요약으로 사용했다. 별로다.

## 3.7. Translation

영어 → 프랑스 , 프랑스 → 영어 인데 별로라고 한다. 데이터 셋에 프랑스 어가 별로 없었다고 한다. ( 10 MB )

## 3.8. Question Answering

일반적인 사실에 대한 질문에 대해서 정답을 얼마나 자주 뽑아 내는가 에 대해서 평가를 했다고 하는데 모델 용량이 중요하다고 한다. 기존의 것과 비교해서 30~50% 성능이 나쁘다고 한다.

## 4. Generalization vs Memorization

비전 데이터 셋인 CIFAR-10에 중복 이미지 문제가 있었고, 일반화 성능에 대해 오버 리포팅 문제가 있었다. WebText 도 비슷한 문제가 있을 수가 있기 때문에 얼마나 테스트 셋에서 학습 데이터 셋이 나타나는 지 분석하는 것이 중요했다.

그래서 Web Text 학습 데이터 셋에 8 그램 토큰을 가지는 Bloom filter(원소가 집합에 속하는지 여부를 검사한느데 사용하는 확률적 자료 구조, 8-gram이 집합에 속해 있는가?를 검사한다.)를 생성 해서 사용 했다.

{{< figure src="/images/gpt2/13.png" title="13" >}}

공통 언어 모델의 테스트 셋은 학습 셋과 1~6% 정도 중복 되고, 평균적으로 3.2%가 겹친다. 많은 데이터 셋이 분할된 학습 셋과 평균 5.9%의 중복을 가지고 있다.

Web Text 학습 데이터 셋과 특정 평가 데이터 셋 간의 데이터 중복은 작지만 중복이 장점으로 작용하기는 한다고 한다. 다른 데이터 셋 같은 경우에는 더 높은 오버 랩이 발생한다고 한다. 

새로운 NLP 데이터셋에 대해 n-gram overlab을 통한 중복 제거 방법을 사용할 것을 본 논문에서 추천하고 있다. 

{{< figure src="/images/gpt2/14.png" title="14" >}}

위 그래프를 보면 GPT-2가 언더 피팅이므로 암기가 held-out 셋에 암기하는 문제가 발생하지는 않았을 거라고 말 할 수 있다고 한다.

# 6. Discussion

- 비지도 학습 태스크가 추가적인 연구가 필요한 분야라고 제시한다.
- GPT-2가 아직 제로샷 성능 결과로 볼때는 사용하기는 좀 부족하다고 한다.
- decaNLP와 GLUE 와 같은 벤치마크에 파인튜닝에 대해 조사를 계획하고 있다