---
title: "Longformer: The Long-Document Transformer 리뷰"
date: 2021-08-17T00:12:31+09:00
draft: false
categories: ["paper-review"]
tags: ["ai", "nlp", "longformer"]
---

# ABSTRACT

셀프 어텐션 때문에 트랜스포머 기반 모델들이 긴 시퀀스를 처리 못한다. 시퀀스 길이가 늘어날 수록 연산량이 크게 증가하기 때문이다. 그래서, 시퀀스 길이에 선형적으로 대응하는 어텐션 메커니즘을 사용하는 롱포머를 제안한다. 롱포머는 기존 셀프 어텐션을 교체(드롭-인)하고 태스크 처리를 위한 글로벌 어텐션과 결합한다. 따라서 수천 이상의 토큰을 가지는 문장을 쉽게 처리할 수 있다. 

기존의 트랜스포머로 긴 시퀀스 처리하는 연구의 검증 방법에 따라 문자 단위의 언어 모델링으로 롱포머를 검증했고 text8과 enwik8 벤치마크에서 SOTA를 달성했다.

기존 다른 연구와 다르게, 추가로 롱포머를 사전학습하고 다양한 다운스트림 태스크에 파인튜닝했다. RoBERTa에 비해 긴 문장 태스크들에 상당한 성능 향상을 보였고 Wiki-Hop과 TriviaQA 벤치마크에서 새로운 SOTA를 달성했다.

긴 문서에서 시퀀스-투-시퀀스 생성 태스크를 할수 있도록 LED(Longformer-Encoder-Decoder)라는 롱포머의 변형을 소개했고 arXiv summarization 데이터셋으로 효과를 증명했다.

# 1 Introduction

트랜스포머는 언어 생성 모델과 언어 이해 모델을 포함한 광범위한 자연어 태스크에서 SOTA를 달성했다. 셀프 어텐션이 전체 시퀀스의 문맥 정보를 잘 캡쳐하기 때문이다. 

문제가 있다. 시퀀스 길이가 증가할 수록 이차 형태($n^2$)로 메모리와 연산이 증가한다. 

롱포머는 셀프 어텐션 연산을 시퀀스 길이에 선형 대응하도록 구조를 수정해서 긴 문서를 잘 처리하게 만들려고 했다.

{{< figure src="/images/longformer/0.png" title="1" >}}

기존의 BERT-스타일 사전 학습 모델은 토큰을 512 길이로 제한을 해서 긴 문장을 분할/축소 해서 처리했기 때문에 긴 문서 분류, 질문/답변, 상호 참조 같은 긴 시퀀스를 다루는 자연어 태스크에서 롱포머는 메리트가 있다. 

파티셔닝을 하는 것은 간단하지만, 파티션 간 정보 손실이 있을 수 있고, 후에 정보 손실을 해결하기 위해 뒷단에 복잡한 구조를 도입하기도 했다.

롱포머는 여러 개의 어텐션 레이어를 사용해서 전체 컨텍스트의 문맥적 표현을 생성할 수 있고, 특정 태스크를 위한 모델 구조의 필요성을 감소시킨다.

기존 연구들에서 긴 시퀀스를 처리하는 트랜스포머의 계산 효율성을 증가시키려고 했지만, autoregressive한 언어 모델링 쪽으로 초점이 맞춰져 있고, 트랜스퍼 러닝에서 문서 단위 NLP 태스크들에 긴 문서 트랜스포머를 적용하려는 시도가 거의 없었다. 

롱포머의 어텐션 메커니즘은 윈도우 방식의 로컬 문맥 셀프 어텐션과 엔드 태스크에서 태스크에 대한 단서를 제공하는 역할을 하는 글로벌 어텐션을 결합하는 방식이다. 

로컬 어텐션이 문맥적 표현을 구축하는데 사용하고, 글로벌 어텐션으로 예측을 위한 전체 시퀀스 표현을 구축한다.

Autoregressive Character-level 언어 모델링으로 검증을 하는데 윈도우 방식과 딜레이티드 어텐션 패턴을 적용한 방식을 검증한다. 32K개의 토큰이 GPU에서 동작할수 있도록 만들어서 text8과 enwik8 벤치마크 데이터셋에서 효과성을 증명했다.

롱포머의 성능 검증으로 기존의 사전 학습 모델의 셀프 어텐션 연산을 교체해서 검증한다. MLM 방식으로 사전 학습한 RoBERTa의 체크포인트에 이어서 사전학습을 한 후, 파인튜닝으로 다운스트림 태스크들에 적용한다. 그 결과 텍스트 분류, QA, 상호 참조 해결의 문서 단위 태스크 들에서 RoBERTa에 비해 좋은 성능을 보였고 그중 두 데이터 셋에서는 SOTA를 달성했다.

마지막으로 오리지널한 트랜스포머 모델과 유사한 인코더-디코더 구조로 롱포머를 변형해서 시퀀스-투-시퀀스 학습에 사용한다. 이 모델을 LED라고 부르고 인코더 네트워크에 롱포머의 어텐션 패턴을 사용한다. 요약 같은 긴 문서의 seq2seq에 사용한다. LED의 효과를 arXiv 요약 데이터셋으로 증명한다.

# 2 Related Work

## Long-Document Transformers

{{< figure src="/images/longformer/1.png" title="2" >}}

두가지 방식의 어텐션이 연구 되고 있다. 

1. left-to-right(ltr) : autoregressive한 언어 모델링에 잘 동작하지만, 양방향 컨텍스트가 요구되는 태스크에는 적절하지는 않다.
2. sparse : 전체 이차 형태의 어텐션 매트릭스 곱을 연산을 회피하는 롱포머의 방식. 딜레이티드 슬라이딩 윈도우 방식의 Sparse(2019)와 롱포머가 유사하지만, 커스텀 CUDA 커널을 구현했다. 그리고 글로벌 어텐션이 있다.

그 밖에 BP-Transformer는 기계 번역. Blockwise는 QA(그리고 짧은 문서)에 특화되고 언어 모델링이 없다.

## Task-specific Models for Long Documents

태스크 별 방식은 BERT 같은 트랜스포머의 512 토큰 제한을 극복하려고 했다. 

- 문서를 잘라내는 방식(가장 간단, 분류에 주로 사용됨)
- 512 길이로 청킹(겹쳐도 됨, 각 청크 처리 후 결합 )
- 투 스테이지 모델(멀티홉, 오픈 도메인 QA, 문서 검색→답변 추출)

위의 방식들은 정보 손실 문제가 있다. 롱포머는 모두 결합해 한번에 다 처리하는 더 간단한 방식이다.

롱포머와 비슷한 아이디어인 local + global 어텐션을 트랜스포머에 사용하는 연구들이 있다.

- ETC - 상대 위치 임베딩 사용, 사전학습에 CPC(Contrastive Predictive Coding 방법론은 Target Class를 직접적으로 추정하지 않고 Target 위치의 벡터와 다른 위치의 벡터를 비교하는 방식) 로스 사용, 글로벌 어텐션에 차이가 있다(독해와 분류 잘한다)
- GMAT - 글로벌 메모리로 입력의 일부의 글로벌한 위치를 제공한다
- BigBird - Sparse 트랜스포머가 시퀀스 함수를 근사하고 full 셀프 어텐션의 이러한 특성을 유지한다는 것을 이론적으로 증명했다

# 3 Longformer

오리지널 트랜스포머 모델의 셀프 어텐션은 $O(n^2)$의 시간 복잡도와 메모리 복잡도를 가진다. 여기서 n은 입력 시퀀스 길이이다. 

롱포머는 "어텐션 패턴"을 사용해서 전체 셀프 어텐션 매트릭스를 희소화한다.(다 쓰지 않는다) 제시된 어텐션 패턴이 긴 시퀀스를 효율적으로 처리하기 위해서 입력 시퀀스를 선형적으로 스케일링할 수 있도록 계산량을 축소한다.

## 3.1 Attention Pattern

### Sliding Window( Local Context )

로컬 문맥의 중요성을 고려해서, 각 토큰을 감싸는 고정된 크기의 윈도우 어텐션을 도입한다. CNN과 유사하다. 여러 층으로 쌓인 윈도우 방식의 어텐션 결과로 큰 리셉티브 필드를 만들어 최상단 레이어는 입력의 모든 위치와 통합된 정보를 담게 한다. 주어진 고정된 윈도우 크기 $w$에서 각 토큰이 양 사이드에서 $\frac{1}{2}w$ 만큼 윈도우가 적용된다. 이 패턴의 계산 복잡도는 $O(n\times w)$이다.  

최상단 레이어의 리셉티브 필드의 크기는 $l \times w$ 다.( 모든 레이어에서 w는 고정되었다고 가정한다 ) 어플리케이션에 따라 각 레이어에서 다른 w를 사용하는 것은 효율성과 모델 표현력을 밸런싱한다.

### Dilated Sliding Window ( Local Context )

계산량은 그대로 리셉티브 필드를 늘리기 위해 슬라이딩 윈도우를 'dilated" 시킨다. 윈도우에 딜레이션 사이즈 d 만큼의 갭을 넣는 것이다. 모든 레이어들에 크기 w, 고정된 크기 d를 사용한다고 가정하면, 리셉티브 필드는 $l \times d \times w$ 이다. 여기서 d의 사이즈는 수천 개의 토큰들에 비해 상대적으로 작다.

멀티 헤드 어텐션에서 각 어텐션 헤드는 서로 다른 어텐션 스코어를 계산한다. 헤드마다 딜레이션 구성을 다르게 하면 성능이 개선된다 것을 발견했다. 딜레이션이 없는 헤드는 로컬 컨텍스트에 포커싱하고, 딜레이션 있는 헤드는 더 긴 컨텍스트에 포커싱하게 한다.

### Global Attention ( Global Context )

언어 모델링에 따라 입력된 시퀀스의 표현이 달라지고 태스크에 따라 보다 더 다양해진다. 

MLM에서는 마스킹된 워드를 예측하기 위해 로컬 문맥을 사용하고 이후 분류를 위해 전체 시퀀스의 표현을 스페셜 토큰(BERT의 경우 [CLS])에서 집계한다. QA에서는 질문과 문서가 결합되고 셀프 어텐션으로 질문을 문서와 비교한다.

미리 선택된 입력 위치에 "글로벌 어텐션"을 추가한다. 중요한 점은 이 어텐션 연산을 대칭으로 만드는 것이다. 글로벌 어텐션을 하는 토큰에 시퀀스의 모든 토큰을 사용하고 모든 토큰의 입력을 사용한다. 

로컬 어텐션과 글로벌 어텐션의 결합을 해도 여전히 복잡도가 $O(n)$이다. 글로벌 어텐션을 적용하는 토큰의 수가 적고 이것이 n의 복잡도와 독립적이기 때문이다. 태스크 별 글로벌 어텐션을 다르게 정의하기 때문에 복잡한 구조를 사용하는 기존의 방식보다 간단하다.

{{< figure src="/images/longformer/2.png" title="3" >}}

### Linear Projections for Global Attention

트랜스포머 모델은 어텐션 스코어를 다음과 같이 계산한다.

{{< figure src="/images/longformer/3.png" title="" >}}

슬라이딩 윈도우 어텐션 - 어텐션 스코어 계산에 $Q_s, K_s, V_s$를 사용하고

글로벌 어텐션을 - 어텐션 스코어를 계산에 $Q_g, K_g, V_g$를 사용한다.

+ 초기화는 각각 동일하게 한다.

## 3.2 Implementation

기존 트랜스포머의 어텐션 스코어 계산에서 가장 비싼 연산은 $QK^T$로 $n^2$이다. 

롱포머에서는 딜레이티드 슬라이딩 윈도우 어텐션이 $QK^T$의 고정된 대각선 수 만큼만 계산한다. 구현에는 밴디드 매트릭스 곱 형태가 필요하다. 그리고 파이토치와 텐서플로우에는 제공하지 않는다.

{{< figure src="/images/longformer/4.png" title="4" >}}

그림 1에서 세개의 서로 다른 구현체를 비교했다.

- loop - 메모리 효율적인 파이토치 구현으로 델레이션 제공하지만 너무 느려서 테스트 밖에 못씀
- chunks - 딜레이티드 아닌 케이스만 지원하고 사전 학습/파인튜닝에만 사용한다
- cuda - TVM을 사용한 커스텀 CUDA 커널 구현으로 언어 모델링에 사용한다

# 4 Autoregressive Language Modeling

입력 시퀀스에서 주어진 이전 토큰들/문자들 에 대해 현재 토큰/문자의 확률 분포를 예측하는 자연어 처리 근본 태스크로 기존의 많은 연구에서 언어 모델링에 사용한다.

## 4.1 Attention Pattern

딜레이티드 슬라이딩 윈도우 어텐션을 사용했다. 레이어에 따라 윈도우 사이즈를 다르게 사용했다. 

작은 윈도우 사이즈에서 레이어를 거칠 때마다 윈도우 사이즈를 키워 최상단 레이어는 전체 시퀀스의 하이 레벨의 표현을 학습하게 한다. 표현력(윈도우 사이즈가 클수록 더 많은 표현)과 효율성(작을 수록 계산량 적어짐)의 밸런싱은 해줘야한다

딜레이티드 슬라이딩 윈도우는 상위 레이어에서 로컬 컨텍스트를 최대한 보존할 수 있도록 두 헤드 정도에 사용한다. 

## 4.2 Experiment Setup

### Training

단계별 학습 과정을 채택해서 여러 학습 단계 별로 어텐션 윈도우 사이즈와 시퀀스 길이를 증가시켰다. 첫번째 단계에서는 짧은 시퀀스 길이와 윈도우 사이즈로 시작해서, 이후 서브 시퀀스 단계에서 윈도우 사이즈와 시퀀스 길이를 두배씩 증가시키면서 학습율은 반감한다. 모델 학습을 다섯 단계로 하고 시작 시퀀스 길이가 2,048로 마지막 단계에서 시퀀스 길이는 23,040 이다. 

### Evaluation

시퀀스 길이를 32,256로 검증한다. 시퀀스를 512 길이 씩 자르고 시퀀스의 마지막 512 토큰들로 성능을 리포트한다.

### 4.2.1 Results

text8과 enwik8 벤치마크 데이터 셋으로 검증에 사용했다. 작은 모델을 사용했을 때, 새로운 SOTA를 BPC 1.10과 1.00로 달성했다. 

{{< figure src="/images/longformer/5.png" title="5" >}}

큰 모델에서는 enwik8 데이터셋만 사용했다. 테이블 3은 롱포머가 트랜스포머-XL 모델에 비해 좋고 Sparse Transformer와 유사하다 혹은 최근의 두배 이상의 파라미터를 가지는 모델에 비해 조금 덜 좋다 정도를 보인다. Adaptive와 Compressive는 사전학습과 파인튜닝이 안되어서 성능이 더 좋긴한데 어차피 가치가 없다.

{{< figure src="/images/longformer/6.png" title="6" >}}

### 4.2.2 Ablation Study

어텐션 패턴 선택의 중요도를 보이기 위해서 실험을 진행한다. 각 구성을 150K 스텝의 한 페이즈로 하고 text8을 사용해서 BPC 성능을 보인다.

테이블 4에서 레이어 별 윈도우 사이즈에 대한 효과를 증명한다. 상위 레이어로 갈 수록 윈도우 사이즈를 증가시키는 것이 성능이 가장 좋았다. 아래는 딜레이션의 효과로 안쓰는 것에 비해서 성능을 개선했다.

{{< figure src="/images/longformer/7.png" title="7" >}}

# 5 Pretraining and Finetuning

현재 많은 NLP 태스크에서 SOTA는 사전학습한 모델을 태스크에 맞춰 파인튜닝 했다. 본 논문에서는 긴 문서 태스크에 적합한 모델을 만들고 싶었다. 그래서 문서 코퍼스에 롱포머를 사전학습 시키고 여섯개 태스크에 파인튜닝했다. 여기서 나온 롱포머 모델은 BERT에 비해 8배 많은 4096개 토큰 시퀀스를 처리할 수 있다.

학습 비용의 문제로 RoBERTa 릴리즈를 계속해서 사전학습 했다. 릴리즈 된 체크포인트에 이어서 어텐션 만 교체하면 되므로 간단하다.

## Attention Pattern

슬라이딩 윈도우 어텐션의 윈도우 사이즈를 512로 사용해 RoBERTa와 같은 계산량이 같다.

## Positional Embeddings

RoBERTa가 기존에 학습된 절대 포지션 임베딩을 512 사이즈를 사용했는데 롱포머는 더 긴 문서를 지원하기 위해 추가 포지션 임베딩을 추가해야 한다. BERT의 어텐션 헤드 분석 결과 로컬 컨텍스트에 대해 강한 학습된 바이어스를 가지고 있다는 것을 근거로 포지션 임베딩을 랜덤 초기화 해서 사용하지 않고 기존의 512 포지션 임베딩을 복사해서 초기화 했다.  간단해 보이지만 매우 효과적이었다. 

{{< figure src="/images/longformer/8.png" title="8" >}}

## Continued MLM Pretraining

컴파일한 긴 문서 코퍼스인 fairseq로 롱포머를 base와 large 두 사이즈로 모델을 학습했다. 

- 두 모델 모두 65K 만큼의 가중치 업데이트
- 시퀀스 길이 4,096에 배치 사이즈 64로 수행했다.
- 최대 학습율이 3e-5 였다.
- 리니어 웜엄이 스텝을 500 이었다.
- 폴리노미얼 디케이를 3으로 적용했다.
- 나머지는 RoBERTa와 동일한 파라미터를 사용했다.

테이블 5에서 copy position embeddings와 2K 그리고 65K에서 BPC가 1.957에서 1.705로 감소한다. 

모델이 더 긴 컨텍스트와 슬라이딩 윈도우 어텐션을 더 잘 활용하도록 학습한다는 것을 증명한다. 

## Frozen RoBERTa Weights

모든 RoBERTa 가중치를 프리징하고 새로운 포지션 임베딩만 학습을 하기도 하는데 짧은 문서에 대한 RoBERTa 성능을 완벽하게 보존하려고 했다. 1.850으로 학습한게 나았지만 전체 다 학습한 1.705보다는 높았다.

# 6 Tasks

QA, coreference resolution 그리고 분류를 포함한 여러 긴 문서 태스크에 롱포머를 적용했다. 테이블 6의 검증 데이터셋은 512 보다 긴 단어 피스를 가진다. 목표는 어텐션 메커니즘의 효과를 검증하는 것이다.

{{< figure src="/images/longformer/9.png" title="9" >}}

베이스라인은 RoBERTa 기반 모델로 가능한 가장 긴 세그먼트로 컨텍스트로 나눠서 RoBERTa 각각 처리해서 나중에 합치는 방식이다. 롱포머 변형은 RoBERTa 셀프 어텐션 메커니즘을 우리의 윈도우 방식의 어텐션을 사용하고 글로벌 어텐션을 추가한다. 

## 6.1 Question answering

세 데이터셋을 사용했다. WikiHop, TriviaQA 그리고 HotpotQA.

기존 BERT를 따라 질문과 문서들을 하나의 긴 시퀀스로 결합하고 롱포머를 사용한다. 그리고 데이터셋에 특화된 예측 레이어를 사용한다. 

WikiHop은 후보를 위한 분류 레이어를 사용했다. TriviaQA는 Clark and Gardner 로스 함수를 사용해서 답변 스팬을 예측한다. 

WikiHop에 대해 글로벌 어텐션을 질문 토큰과 후보 답변에 적용했다. TriviaQA에서는 질문 토큰에 글로벌 어텐션을 적용한다. 

HotpotQA는 멀티홉 QA 데이터셋이다. 위키피디아 문단 10개에서 증거 문장들과 답변 스팬을 추출한다. 10개 중 2개는 관련성이 있고 나머지는 오답이다. 스테이지 두개를 가지는 모델을 사용한다. 이전과 컨텍스트 구성은 같고 태스크 전용 예측 레이어를 사용한다. 관련성 있는 문단 예측, 증거 문장들, 답변 스팬들과 질문 타입들(yes/no/span)을 함께 예측하도록 학습한다. 이 모델이 복잡한 태스크에 특화된 구조를 사용하는 최근 SOTA 모델들에 비해 간단하다. 

## 6.2 Coreference Resolution

OntoNotes 데이터셋을 사용하고 Lee et al(2018)의 시스템 수정 모델인 , Joshi et al(2019)로 ELMO를 BERT로 교체했다. 롱포머 시스템은 RoBERTa를 교체하고 시퀀스 길이를 확장했다. 이 태스크에서 글로벌 어텐션은 사용하지 않았다.

## 6.3 Document Classification

IMDB와 Hyperpartisan 데이터셋으로 실험을 했다. IMDB는 영화 리뷰들로 구성된 감정 분류 데이터셋이고 대부분이 짧다. 13.6%만 512 워드피스보다 크다. Hyperpartisan의 문서는 상대적으로 길고, 645건의 문서만 길이가 짧다. [CLS] 토큰에 글로벌 어텐션을 사용했다.

## 6.4 Results

### Main Result

테이블 7은 모든 파인튜닝 실험의 결과를 요약했다. RoBERTa 베이스라인에 비해 훌륭히 상당히 좋았다. 

성능은 특히 WikiHop과 Hyperpartisan과 같은 긴 컨텍스트를 요구하는 태스크에서 확실했다. 

TriviaQA는 로컬 컨텍스트가 질문에 답변하기에 충분한 경우가 많아서 개선이 중간 정도 였다.

HotpotQA의 경우, 팩트 보조 감독이 모델이 관련성 있는 컨텍스트를 쉽게 찾게하고 로컬 컨텍스트에 초점을 맞추어서 개선이 적게 된다. 이것은 중간 추론 체인에 대한 원거리 감독만을 포함하는 WikoHop과 반대로, 전체 컨텍스트에 대한 추론에 의해 우리의 접근 방식이 더 좋다.

IMDB와 OntoNotes 데이터셋들은 성능 개선이 적다. IMDB는 문서가 짧아서 개선이 더 적고 OntoNotes는 두 멘션 간 거리가 일반적으로 작기 때문에 작은 청크를 별도로 처리하는 베이스 라인이 교차 청크 상호작용을 고려하지 않아도 멘션을 합칠 수 있다.

{{< figure src="/images/longformer/10.png" title="10" >}}

### Longformer-large for QA

테이블 8에서 WikiHop과 TriviaQA는 새로운 SOTA 결과를 롱포머-large로 달성했다. 테이블 9는 HotpotQA는 더 자세한 결과를 보인다. 롱포머는 공개 리더보드에서 두번째로 GNN이 첫번째다. GNN을 안쓰고는 최고의 성능이다.

{{< figure src="/images/longformer/11.png" title="11" >}}

{{< figure src="/images/longformer/12.png" title="12" >}}

## 6.5 Ablation on WikiHop

테이블 10은 WikiHop에 ablation study를 한 결과이다. 모든 결과들은 베이스가 롱포머고 명시된 것을 제외하고는 동일한 하이퍼 파라미터로 5 epoch 동안 파인튜닝 했다. 

{{< figure src="/images/longformer/13.png" title="13" >}}

# 7 Longformer-Encoder-Decoder(LED)

- 인코더에서 롱포머의 local+global 어텐션 패턴을 사용했다.
- 디코더가 전체 인코딩된 토큰과 이전에 디코딩된 위치에 풀 셀프 어텐션을 사용하는 형태로 구성했다.
- BART의 레이어의 수와 히든 사이즈를 그대로 따랐고 추가로 LED의 파라미터들을 초기화 했다.

16K 토큰으로 포지션 임베딩을 확장했다. 마찬가지로 BART의 1K를 복사해서 반복해서 초기화 했다. LED-base와 LED-large 모델 두개가 있다.

ArXiv 요약 데이터셋을 사용해서 LED를 검증했다. 문서 길이의 90% 퍼센타일은 14.5K 토큰이어서 검증용으로 적절했다. 

인코더가 문서를 읽고 디코더가 요약 결과를 생성한다. 인코더에서 1024 토큰 크기 만큼의 윈도우로 로컬 어텐션을 사용했고 첫 <s> 토큰에 글로벌 어텐션을 했다. 디코더는  전체 인코더와 이전에 디코딩된 위치에 풀 어텐션을 사용한다. 추가로 학습에 티처 포싱을 사용했고 추론 시에 빔 서치를 사용했다.

테이블 11은 결과를 보인다. LED는 arXiv에 SOTA를 달성했다. 사전 학습 혹은 태스크 특화 초기화 없이, 긴 입력값을 처리로 BigBird 보다 좋은 성능을 보인다. 

그림 3은 시퀀스 길이의 중요성을 설명한다.

{{< figure src="/images/longformer/14.png" title="14" >}}

{{< figure src="/images/longformer/15.png" title="15" >}}

# 8 Conclusion and Future Work

롱포머를 제시했다. 롱포머는 트랜스포머 기반 모델로 긴 시퀀스를 분할하거나 줄이지 않아도 되고 복잡한 구조를 추가하지 않아도 된다. 

로컬과 글로벌 컨텍스트 정보를 결합하는 어텐션 패턴을 적용했다.

text8과 enwik8을 사용한 문자 수준 언어 모델링에서 SOTA를 달성했다.

사전학습 되었을 때 WikiHop과 TriviaQA 같은 데이터셋을 사용하는 긴 문서 태스크에서 새로운 SOTA를 달성했다.

LED를 제시했는데  arXiv 문서 요약 태스크에서 SOTA를 달성했다.

LED를 위한 다른 사전학습 오브젝티브를 연구 + 시퀀스 길이를 늘이기 위한 연구 + 다른 적합한 태스크를 찾는 연구를 할 예정이다.

# 참고 논문 리뷰

- [https://medium.com/@eyfydsyd97/논문-리뷰-longformer-the-long-document-transformer-e9ade1980536](https://medium.com/@eyfydsyd97/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-longformer-the-long-document-transformer-e9ade1980536)
- [https://littlefoxdiary.tistory.com/47](https://littlefoxdiary.tistory.com/47)
- [https://haebinshin.github.io/Longformer](https://haebinshin.github.io/Longformer)
- [https://wiki.beomi.net/longformer.html](https://wiki.beomi.net/longformer.html)